{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef539438",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e4fd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "example:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list=glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus=[]\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file,'r') as f:\n",
    "        raw=f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\",len(raw_corpus))\n",
    "print('example:\\n',raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b0353",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ce41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    import re\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"(\\[.*\\])\", '', sentence) #2\n",
    "    sentence = re.sub(r\"([?.!])\", r\" \\1 \", sentence) # 3\n",
    "    sentence = re.sub(r\"('[s|t])\", r\" \\1\", sentence) # 4\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 5\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!']+\", \" \", sentence) # 6\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #5\n",
    "    sentence = sentence.strip() # 7\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea82a55",
   "metadata": {},
   "source": [
    "1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "2. 특수문자 양쪽에 공백을 넣고\n",
    "3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "5. 다시 양쪽 공백을 지웁니다\n",
    "6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c144cc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<start> now i've heard there was a secret chord <end>\",\n",
       " '<start> that david played and it pleased the lord <end>',\n",
       " \"<start> but you don 't really care for music do you ? <end>\",\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth the fifth <end>',\n",
       " '<start> the minor fall the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "   \n",
    "    if len(sentence) == 0: continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae43ba6",
   "metadata": {},
   "source": [
    "* 빈 문장 제외하고 정제된 문장을 corpus에 넣는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b5fc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   49  147 ...    0    0    0]\n",
      " [   2   15 2677 ...    0    0    0]\n",
      " [   2   34    6 ...    3    0    0]\n",
      " ...\n",
      " [ 127   21    8 ...    9 1027    3]\n",
      " [8413    5   36 ... 1293  645    3]\n",
      " [   2    6   33 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f357c2acdf0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175986, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    \n",
    "    tokenizer=tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=12000,\n",
    "    filters='',\n",
    "    oov_token=\"<unk>\")\n",
    "    \n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor=tokenizer.texts_to_sequences(corpus)\n",
    "    #입력데이터의 시퀀스 길이를  padding으로 맞춰줌, 최대 토큰 길이는 15\n",
    "    tensor=tf.keras.preprocessing.sequence.pad_sequences(tensor,maxlen=15,padding='post')\n",
    "    \n",
    "        \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor,tokenizer\n",
    "\n",
    "tensor,tokenizer=tokenize(corpus)\n",
    "tensor.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a3920",
   "metadata": {},
   "source": [
    "* corpus를 텐서로 변환\n",
    "* 토큰의 개수를 15로 제한시켜줌 (maxlen=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f8959",
   "metadata": {},
   "source": [
    "# 데이터 분리(훈련/검증/테스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287f676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140788, 14)\n",
      "Target Train: (140788, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]  \n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=32)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbca240a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 14), (128, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input) \n",
    "BATCH_SIZE = 128       \n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset=tf.data.Dataset.from_tensor_slices((src_input,tgt_input))\n",
    "dataset=dataset.shuffle(BUFFER_SIZE)\n",
    "dataset=dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb0808",
   "metadata": {},
   "source": [
    "# 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c1a5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_size,hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding=tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
    "        self.rnn1=tf.keras.layers.LSTM(hidden_size,return_sequences=True)\n",
    "        self.rnn2=tf.keras.layers.LSTM(hidden_size,return_sequences=True)\n",
    "        self.linear=tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self,x):\n",
    "        out=self.embedding(x)\n",
    "        out=self.rnn1(out)\n",
    "        out=self.rnn2(out)\n",
    "        out=self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeda015",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30c4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.num_words + 1 \n",
    "embedding_size=256\n",
    "hidden_size=2048\n",
    "model=TextGenerator(vocab_size,embedding_size,hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118ca26",
   "metadata": {},
   "source": [
    "* vocab_size는 총 단어 개수에 pad:0 이 포함된 크기\n",
    "* embedding_size는 단어가 추상적으로 표현되는 크기\n",
    "* hidden_size 는 모델에 얼마나 많은 일꾼을 두느냐(많으면배가 산으로 갈 수도 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85e77a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-1.44935155e-04, -2.05210614e-04,  6.44014290e-05, ...,\n",
       "          5.18250636e-05, -4.36777518e-05,  1.60979194e-04],\n",
       "        [-2.25001946e-04, -1.82580607e-05,  3.18568840e-04, ...,\n",
       "          1.71400330e-04, -1.93362343e-04, -5.52994970e-05],\n",
       "        [-2.18223577e-04, -1.38959702e-04,  3.72762734e-04, ...,\n",
       "         -7.35447538e-05, -1.90920531e-04, -2.88842071e-04],\n",
       "        ...,\n",
       "        [-7.24552548e-04,  4.37345181e-04,  8.55459541e-04, ...,\n",
       "         -2.62384856e-04, -6.82816841e-04, -1.03099307e-03],\n",
       "        [-7.88262987e-04,  4.92267311e-04,  7.69754639e-04, ...,\n",
       "         -2.02371913e-04, -4.71976382e-04, -8.50314915e-04],\n",
       "        [-7.16526585e-04,  5.99232619e-04,  9.28453635e-04, ...,\n",
       "         -1.06380598e-04, -6.30124821e-04, -7.08819716e-04]],\n",
       "\n",
       "       [[-1.44935155e-04, -2.05210614e-04,  6.44014290e-05, ...,\n",
       "          5.18250636e-05, -4.36777518e-05,  1.60979194e-04],\n",
       "        [-2.35080879e-04, -5.90304699e-05, -1.97009704e-05, ...,\n",
       "         -4.82746764e-05, -1.87205471e-04, -7.00110977e-05],\n",
       "        [-4.36597300e-04,  1.70454477e-05, -2.02898693e-04, ...,\n",
       "         -1.39535434e-04, -1.05155392e-04, -2.41575603e-04],\n",
       "        ...,\n",
       "        [ 6.33958203e-04,  9.03945824e-04, -1.01173115e-04, ...,\n",
       "         -4.35831826e-05, -2.40804773e-04, -1.01651298e-03],\n",
       "        [ 8.19431851e-04,  8.89840361e-04,  2.69062497e-04, ...,\n",
       "          8.41942674e-05, -6.64674793e-04, -9.82580124e-04],\n",
       "        [ 9.88241518e-04,  8.55759892e-04,  6.48175832e-04, ...,\n",
       "          2.52110825e-04, -1.16820249e-03, -8.77804123e-04]],\n",
       "\n",
       "       [[-1.44935155e-04, -2.05210614e-04,  6.44014290e-05, ...,\n",
       "          5.18250636e-05, -4.36777518e-05,  1.60979194e-04],\n",
       "        [-1.11172871e-04, -3.06545640e-04,  5.63478716e-05, ...,\n",
       "          9.76749361e-05, -2.43603979e-04,  2.19163499e-04],\n",
       "        [-2.49106524e-04, -6.65579108e-04,  1.20161632e-04, ...,\n",
       "         -2.34703639e-05, -2.07380697e-04,  4.13619200e-05],\n",
       "        ...,\n",
       "        [ 3.97706695e-04,  1.33772910e-05,  1.62631879e-03, ...,\n",
       "          6.00041240e-04, -1.93673989e-03, -1.73705295e-04],\n",
       "        [ 6.50853093e-04,  1.44996564e-04,  1.71847595e-03, ...,\n",
       "          8.44653288e-04, -2.43188301e-03, -4.66566999e-05],\n",
       "        [ 8.79057450e-04,  2.45659903e-04,  1.74743566e-03, ...,\n",
       "          1.08190137e-03, -2.87787057e-03,  9.49697787e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.44935155e-04, -2.05210614e-04,  6.44014290e-05, ...,\n",
       "          5.18250636e-05, -4.36777518e-05,  1.60979194e-04],\n",
       "        [-4.29831125e-04, -2.38113513e-04,  7.44311910e-05, ...,\n",
       "         -1.69608160e-04,  1.78744114e-04,  1.95753310e-04],\n",
       "        [-2.42831782e-04, -2.45316740e-04,  2.62220652e-04, ...,\n",
       "         -4.29283391e-04,  4.13309725e-04,  2.02015741e-04],\n",
       "        ...,\n",
       "        [ 5.63033798e-04,  8.22758186e-04, -1.08351800e-04, ...,\n",
       "         -1.89556653e-04, -2.59377615e-04, -1.29140870e-04],\n",
       "        [ 7.19026895e-04,  8.57971085e-04,  3.13878409e-04, ...,\n",
       "         -4.60321826e-06, -7.38192000e-04, -9.00642626e-05],\n",
       "        [ 8.80879874e-04,  8.61903478e-04,  6.78055629e-04, ...,\n",
       "          2.09665319e-04, -1.25616463e-03, -3.28254064e-05]],\n",
       "\n",
       "       [[-1.44935155e-04, -2.05210614e-04,  6.44014290e-05, ...,\n",
       "          5.18250636e-05, -4.36777518e-05,  1.60979194e-04],\n",
       "        [-1.89298749e-04, -3.44185333e-04,  1.53714296e-04, ...,\n",
       "         -2.39820918e-04,  2.28635417e-05,  3.65089218e-04],\n",
       "        [-4.65132995e-04, -3.45774635e-04,  3.70473572e-04, ...,\n",
       "         -3.56050121e-04,  2.87333009e-04,  2.92003591e-04],\n",
       "        ...,\n",
       "        [-2.70897697e-04, -5.63134963e-04,  9.11124924e-04, ...,\n",
       "         -7.89381331e-04,  1.83597964e-04,  6.30801311e-04],\n",
       "        [ 1.76570047e-05, -3.34903714e-04,  1.17546262e-03, ...,\n",
       "         -4.76620160e-04, -2.93501333e-04,  6.05055713e-04],\n",
       "        [ 3.06260510e-04, -1.15076968e-04,  1.43264700e-03, ...,\n",
       "         -1.47374696e-04, -8.46252602e-04,  5.87746326e-04]],\n",
       "\n",
       "       [[ 5.24936913e-05,  2.16456945e-04,  2.08103214e-04, ...,\n",
       "         -7.97558459e-06,  1.75103251e-05,  9.26057328e-05],\n",
       "        [ 2.42475930e-04,  2.45543342e-04,  1.31676177e-04, ...,\n",
       "         -1.56770955e-04, -1.73060558e-04, -1.02136997e-04],\n",
       "        [ 3.18635488e-04,  3.95830692e-04,  1.72646294e-04, ...,\n",
       "         -2.76639068e-04, -3.11295647e-04, -3.83848936e-04],\n",
       "        ...,\n",
       "        [-1.68691063e-03,  4.43137658e-04,  2.59897672e-04, ...,\n",
       "          2.35312782e-05,  4.02048667e-04, -1.84569435e-04],\n",
       "        [-1.67855772e-03,  5.03036485e-04,  4.50676482e-04, ...,\n",
       "         -6.20630453e-05,  3.66640626e-04, -1.26964413e-04],\n",
       "        [-1.69623073e-03,  5.19800815e-04,  3.60781763e-04, ...,\n",
       "         -2.82657202e-05,  2.20657530e-04, -6.20257633e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773adad",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea62675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1100/1100 [==============================] - 393s 355ms/step - loss: 3.4750 - val_loss: 3.1076\n",
      "Epoch 2/10\n",
      "1100/1100 [==============================] - 316s 287ms/step - loss: 2.9198 - val_loss: 2.8331\n",
      "Epoch 3/10\n",
      "1100/1100 [==============================] - 316s 287ms/step - loss: 2.6006 - val_loss: 2.6551\n",
      "Epoch 4/10\n",
      "1100/1100 [==============================] - 317s 288ms/step - loss: 2.2936 - val_loss: 2.5280\n",
      "Epoch 5/10\n",
      "1100/1100 [==============================] - 316s 288ms/step - loss: 2.0039 - val_loss: 2.4356\n",
      "Epoch 6/10\n",
      "1100/1100 [==============================] - 317s 288ms/step - loss: 1.7393 - val_loss: 2.3800\n",
      "Epoch 7/10\n",
      "1100/1100 [==============================] - 316s 287ms/step - loss: 1.5120 - val_loss: 2.3491\n",
      "Epoch 8/10\n",
      "1100/1100 [==============================] - 317s 288ms/step - loss: 1.3257 - val_loss: 2.3461\n",
      "Epoch 9/10\n",
      "1100/1100 [==============================] - 316s 288ms/step - loss: 1.1814 - val_loss: 2.3691\n",
      "Epoch 10/10\n",
      "1100/1100 [==============================] - 317s 288ms/step - loss: 1.0791 - val_loss: 2.3992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f34d0637820>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss,optimizer=optimizer)\n",
    "\n",
    "model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val),epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7d1d4",
   "metadata": {},
   "source": [
    "# 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68946946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer,init_sentence='<start>',max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "       \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "        \n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "923c5ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you raise my hand and we kissed in the moonlight <end> '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you raise\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5e2c0",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4972b104",
   "metadata": {},
   "source": [
    "* 토큰 길이 조절: tf.keras.preprocessing.sequence.pad_sequences() 의 maxlen으로 조절할 수 있었다.\n",
    "\n",
    "\n",
    "* val_loss 를 조절하기위해서 각종 하이퍼파라미터들을 조절했는데 2.3언저리에서 안떨어진다. 더군다나 7epoch 이후에는 훈련모델에 과대적합이\n",
    "  되는건지 val_loss의 수치가 다시 올라간다. \n",
    "  \n",
    "  \n",
    "* 그리드서치를 배우고 써먹으려고 시도해봤으나 오류가 자꾸만 뜬다. 보통 그리드서치에 들어가는 모델의 예시로는 의사결정나무,랜덤포레스트 등등 만들어진 모델로 쓰긴하는데... 내가 만든 모델로는 그리드 서치가 안되는건가 의문이 든다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
